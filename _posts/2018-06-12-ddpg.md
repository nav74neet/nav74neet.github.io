---
layout: post
title:  "Learning to walk using reinforcement learning"
date:   2018-06-12
categories: [rl]
description: Bipedal walking using Deep Deterministic Policy Gradient.
tags: reinforcement learning, ddpg, bipedal walking,
---
## Caution: This post is under-construction!  
## Intro  
<p style="text-align:justify">Walking is a very basic and indispensable action that is embedded in our daily life. We need to walk to traverse from one place to another. Although it seems to be obvious and too simple, its importance and the level of complexity is taken for granted quite often.</p>
<p style="text-align:justify">But the same cannot be said in the case of a walking robot. There are lots of physical constraints associated with the model and the environment the robot interacts with. This requires complex levels of control theory and accurate mathematical computations. But there's an alternative solution to the problem of bipedal walking robot - Reinforcement Learning.</p>

## History - a research survey
<p style="text-align:justify">While working at the Computational Intelligence Laboratory, at the Aerospace Engineering Division, IISc, I and my colleague <a href="https://arunkrweb.github.io/" class="md-link">Arun Kumar</a> decided to explore the domain of machine learning for applications related to robotics. We were fascinated by the problem of bipedal walking robots and the challenges associated with it's dynamics and stability.</p>

<div class="row">
  <div class="col-md-5">
      <img class="rimg" src="{{ site.github.url }}/media/blog/human_inputs.png"/>
      <figcaption><font size="0.5">Human walking inputs.</font></figcaption>
  </div>
  <div class="col-md-7">
        <p style="text-align:justify">
          Our initial approaches were inclined towards the classical approach of <strong>canonical walking functions</strong>, particularly inspired by the works of <a href="http://ames.gatech.edu/" class="md-link"> Dr. Aron D Ames</a> and his team at <a href="https://www.gatech.edu/" class="md-link">Georgia Tech</a> & <a href="http://www.bipedalrobotics.com/" class="md-link">AMBER Lab at Caltech</a>, related to <a href="http://ames.gatech.edu/ijbbr_2014.pdf" class="md-link">human inspired walking</a> frameworks for bipedal walking robots. Canonical walking functions are based on the human gait outputs such as knee & hip joint angles and horizontal distance of the hip during the gait, to formulate the controllers for the robot.
        </p>
    </div>
</div>

<p style="text-align:justify">We also decided to delve more into biologically inspired controllers and came across <strong>CPG</strong>, acronym for <strong>Central Pattern Generators</strong>. So what's a CPG?. Basically they are biological neuronal circuits that produce rhythmic outputs (even in the absense of rhythmic input or sensory feedback). They are the source of the tightly-coupled patterns of neural activity that drive rhythmic motions, for example walking</p>
<div class="row">
<div class="col-md-7">
<p style="text-align:justify">We were heavily influenced by the work of <a href="http://www.robots.ox.ac.uk/~gunes/" class="md-link">Atılım Güneş Baydin</a> on "<a href="https://arxiv.org/pdf/0801.0830.pdf" class="md-link">Evolution of central pattern generators for the control of a five-link bipedal walking mechanism</a>". On a brief note, this paper describes the approach of determining the connectivity and oscillatory parameters of a CPG network through <strong>genetic algorithm(GA)</strong> optimization techniques. The CPG model adopted is that of a <a href="https://link.springer.com/article/10.1007/s00422-011-0432-z" class="md-link">Matsuoka's half-center oscillator model</a> shown here.</p>
</div>
<div class="col-md-5">
<img class="center" src="{{ site.github.url }}/media/blog/matsuoka_cpg.png"/>
<figcaption><font size="0.5">Half-center oscillator model of CPG.</font></figcaption>
</div>
</div>

## Reinforcement learning approach.
<p style="text-align:justify">Humans rely on learning from interaction, repeated trial and errors with small variations  and  finding out what works and what not. Let’s consider an example of a child learning to walk. It tries out various possible movements. It may take several days before it stands stably on its feet let alone walking. In the process of learning to walk, the child is punished by falling and rewarded for moving forward. 
	<div class="col-md-5">
      <img class="rimg" src="{{ site.github.url }}/media/blog/baby.gif"/>
	</div>
This rewarding system is inherently built into human beings that motivate us to do actions that garner positive rewards (e.g.,happiness) and discourage the actions that account for bad rewards(e.g.,falling, getting hurt, pain, etc.). This kind of reward based learning has been modelled mathematically and is called <b>Reinforcement learning</b>. A typical reinforcement learning setting consists of an agent interacting with the environment <b><i>E</i></b>. After each time step <b><i>t</i></b>, the agent receives an observation <b><i>s<sub>t</sub></i></b> from the environment and takes action <b><i>a<sub>t</sub></i></b> for which it recieves a scalar reward <b><i>r<sub>t</sub></i></b> & next state <b><i>s<sub>t+1</sub></i></b>.</p>

### Deep Determinsitic Policy Gradient.
<p style="txt-align:justify">In 2016, researchers from Google DeepMind, London, published their results related to "<b><a href="https://arxiv.org/pdf/1509.02971.pdf" class="md-link">Continuos Control with Deep Reinforcement Learning</a></b>" in ICLR. The learning framework is based on the <b>actor-critic network</b></p> 
<p>
<img src="https://nav74neet.github.io/media/blog/ac_network.jpg" alt="AC" align="center">
<figcaption><font size="0.5" align="center">Actor-Critic network model.</font></figcaption>
</p>

### Initial learning phase.
<div class="row">
<div class="col-md-7">
<p style="text-align:justify"> At the initial stages of the learning to walk, the robot takes baby steps and eventually fails a lot of times by falling down. The reward function is modelled accordingly by this inference so that the robot moves forward while maintaining constant waist-height from the ground to maximize the reward for each iteration.
</p>
</div>
<div class="col-md-5">
      <img class="rimg" src="{{ site.github.url }}/media/biped_training.gif"/>
      <figcaption><font size="0.5">Initial learning phase.</font></figcaption>
  </div>
</div>


<!-- <img src="https://nav74neet.github.io/media/biped_training.gif" alt="DDPG" style="border:2px solid black;" align="middle">
<figcaption><font size="0.5" align="center">The initial learning phase.</font></figcaption>
</p> -->

## Results & comparison with an actual human gait.
<div class="row">
    <div class="col-md-7">
      <p style="text-align:justify"> Our model learned to generate a stable walking gait after roughly over 41 hours of training (using a Nvidia GeForce GTX 1050 Ti GPU). Though the problem of convergence of the learning still persists with the DDPG algorithm, we were able to get a bipedal gait which was similar to that of a human's. The human gait was synthezised & derived using optical motion capture camera setup at the Advanced Flight Simulation Laboratory, Dept. of Aerospace Engineering, IISc. The half-body marker placement convention was adopted while recording the subject's walking gait.</p> 
    </div>
  <div class="col-md-5">
      <img class="rimg" src="{{ site.github.url }}/media/blog/motive.gif"/>
      <figcaption><font size="0.5">Motion capture recording of a human walking gait.</font></figcaption>
  </div>
</div>

<p style="text-align:justify"> The hip & knee joint rotations of the human subject and the bipedal walker were gathered and the subsequent plots were generated, depicting similarities in patterns.</p>

<!-- <div class="row">
<div class="col-md-5">
      <img class="rimg" src="{{ site.github.url }}/media/blog/motive.gif"/>
</div> 
</div> -->

## Research outputs & future work

<p style="text-align:justify"><b>[1]</b> The research manuscript detailing our reinforcement learning approach for bipedal walking robot & corresponding results have been documented and can be accessed here: <b>Kumar, Arun, Navneet Paul, and S. N. Omkar. "<a href="https://arxiv.org/abs/1807.05924" class="md-link">Bipedal Walking Robot using Deep Deterministic Policy Gradient.</a>" arXiv:1807.05924 (2018)</b>.</p> 

<p style="text-align:justify"><b>[2]</b> Simulation software framework for the planar bipedal walking robot is opensourced through GitHub <b><a href="https://github.com/nav74neet/ddpg_biped" class="md-link">here</a></b>. You can try out recent state of the art RL algorithms such as <a href="https://blog.openai.com/openai-baselines-ppo/" class="md-link">PPO</a> or  <a href="https://arxiv.org/abs/1502.05477" class="md-link">TRPO</a> for generating walking patterns for the biped robot.</p>

<!-- <p style="text-align:justify">The reason we opted not to go forward with CPG based controllers was infact due to the random pattern generated by the network which did not account for the stability of the system. To consider the factor of dynamic stability of the system, we incorporated a balancing pendulum on top of the waist section of the walker with roll, pitch & yaw (to act as a spine). The balancing control would be taken care of by reinforcement learning where, the top pendulum tries to figure out the optimum pitch, yaw & roll values to stay upright. The CPG network will deal with the generation of walking pattern for the lower torso. But it was the case of easier said than done. </p>  -->
