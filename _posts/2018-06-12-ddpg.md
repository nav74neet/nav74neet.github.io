---

layout: post
title:  "Learning to walk using reinforcement learning"
date:   2018-06-12
categories: [rl]
description: Bipedal walking using Deep Deterministic Policy Gradient.
tags: reinforcement learning, ddpg, bipedal walking,

---

<p>
<div class="image-wrapper">
                <a class ="image-popup" href="https://nav74neet.github.io/media/blog/walking.png" title="walking">
                    <img src="https://nav74neet.github.io/media/blog/walking.png" alt="walking" align="middle">
                    <center>
                <p class="image-caption" style="font-size:10px; text-align:center;">
                    [image source: Eric Frank, <a href="https://eng.uber.com/deep-neuroevolution/" class="md-link">Uber AI Labs</a>].
                </p>
                </center>
                </a>
</div>
</p>

## Intro  
<p style="text-align:justify; font-family: 'Merriweather', 'Hiragino Sans GB', 'Microsoft YaHei', 'WenQuanYi Micro Hei', sans-serif;">Walking is a very basic and indispensable action that is embedded in our daily life. We need to walk to traverse from one place to another. Although it seems to be obvious and too simple, its importance and the level of complexity is taken for granted quite often.</p>
<p style="text-align:justify; font-family: 'Merriweather', 'Hiragino Sans GB', 'Microsoft YaHei', 'WenQuanYi Micro Hei', sans-serif;">But the same cannot be said in the case of a walking robot. There are lots of physical constraints associated with the model and the environment the robot interacts with. This requires complex levels of control theory and accurate mathematical computations. But there's an alternative solution to the problem of bipedal walking robot - Reinforcement Learning.</p>

## History - a research survey
<p style="text-align:justify; font-family: 'Merriweather', 'Hiragino Sans GB', 'Microsoft YaHei', 'WenQuanYi Micro Hei', sans-serif;">While working at the Computational Intelligence Laboratory, at the Aerospace Engineering Division, IISc, I and my colleague <a href="https://arunkrweb.github.io/" class="md-link">Arun Kumar</a> decided to explore the domain of machine learning for applications related to robotics. We were fascinated by the problem of bipedal walking robots and the challenges associated with it's dynamics and stability.</p>

<div class="row">
  <div class="col-md-5">
      <img class="rimg" src="{{ site.github.url }}/media/blog/human_inputs.png"/>
      <figcaption><font size="0.5">Fig.1 Human walking outputs.</font></figcaption>
  </div>
  <div class="col-md-7">
        <p style="text-align:justify; font-family: 'Merriweather', 'Hiragino Sans GB', 'Microsoft YaHei', 'WenQuanYi Micro Hei', sans-serif;">
          Our initial approaches were inclined towards the classical approach of <strong>canonical walking functions</strong>, particularly inspired by the works of <a href="http://ames.gatech.edu/" class="md-link"> Dr. Aron D Ames</a> and his team at <a href="https://www.gatech.edu/" class="md-link">Georgia Tech</a> & <a href="http://www.bipedalrobotics.com/" class="md-link">AMBER Lab at Caltech</a>, related to <a href="http://ames.gatech.edu/ijbbr_2014.pdf" class="md-link">human inspired walking</a> frameworks for bipedal walking robots. Canonical walking functions are based on the human gait outputs such as knee & hip joint angles and horizontal distance of the hip during the gait, to formulate the controllers for the robot.
        </p>
    </div>
</div>

<p style="text-align:justify; font-family: 'Merriweather', 'Hiragino Sans GB', 'Microsoft YaHei', 'WenQuanYi Micro Hei', sans-serif;">We also decided to delve more into biologically inspired controllers and came across <strong>CPG</strong>, acronym for <strong>Central Pattern Generators</strong>. So what's a CPG?. Basically they are biological neuronal circuits that produce rhythmic outputs (even in the absense of rhythmic input or sensory feedback). They are the source of the tightly-coupled patterns of neural activity that drive rhythmic motions, for example walking</p>
<div class="row">
<div class="col-md-7">
<p style="text-align:justify; font-family: 'Merriweather', 'Hiragino Sans GB', 'Microsoft YaHei', 'WenQuanYi Micro Hei', sans-serif;">We were heavily influenced by the work of <a href="http://www.robots.ox.ac.uk/~gunes/" class="md-link">Atılım Güneş Baydin</a> on "<a href="https://arxiv.org/pdf/0801.0830.pdf" class="md-link">Evolution of central pattern generators for the control of a five-link bipedal walking mechanism</a>". On a brief note, this paper describes the approach of determining the connectivity and oscillatory parameters of a CPG network through <strong>genetic algorithm(GA)</strong> optimization techniques. The CPG model adopted is that of a <a href="https://link.springer.com/article/10.1007/s00422-011-0432-z" class="md-link">Matsuoka's half-center oscillator model</a> shown here.</p>
</div>
<div class="col-md-5">
<img class="center" src="{{ site.github.url }}/media/blog/matsuoka_cpg.png"/>
<figcaption><font size="0.5">Fig.2 Half-center oscillator model of CPG.</font></figcaption>
</div>
</div>

## Reinforcement learning approach.
<p style="text-align:justify; font-family: 'Merriweather', 'Hiragino Sans GB', 'Microsoft YaHei', 'WenQuanYi Micro Hei', sans-serif;">Humans rely on learning from interaction, repeated trial and errors with small variations  and  finding out what works and what not. Let’s consider an example of a child learning to walk. It tries out various possible movements. It may take several days before it stands stably on its feet let alone walking. In the process of learning to walk, the child is punished by falling and rewarded for moving forward.
	<div class="col-md-5">
      <img class="rimg" src="{{ site.github.url }}/media/blog/baby.gif"/>
	</div>
</p>
<p style="text-align:justify; font-family: 'Merriweather', 'Hiragino Sans GB', 'Microsoft YaHei', 'WenQuanYi Micro Hei', sans-serif;">This rewarding system is inherently built into human beings that motivate us to do actions that garner positive rewards (e.g.,happiness) and discourage the actions that account for bad rewards(e.g.,falling, getting hurt, pain, etc.). This kind of reward based learning has been modelled mathematically and is called <b>Reinforcement learning</b>. A typical reinforcement learning setting consists of an agent interacting with the environment <b><i>E</i></b>. After each time step <b><i>t</i></b>, the agent receives an observation <b><i>s<sub>t</sub></i></b> from the environment and takes action <b><i>a<sub>t</sub></i></b> for which it recieves a scalar reward <b><i>r<sub>t</sub></i></b> & next state <b><i>s<sub>t+1</sub></i></b> as seen in figure below.
</p>
<center>
            <div class="image-wrapper">
                <a class ="image-popup" href="https://nav74neet.github.io/media/blog/rl.png" title="RL">
                    <img src="https://nav74neet.github.io/media/blog/rl.png" alt="RL" align="middle">
                </a>
                <center>
                <p class="image-caption" style="font-size:10px; text-align:center;">
                    Fig.3 Agent-environment interaction in RL setting.
                </p>
                </center>
            </div>
</center>

### Deep Determinsitic Policy Gradient.

<p style="txt-align:justify; font-family: 'Merriweather', 'Hiragino Sans GB', 'Microsoft YaHei', 'WenQuanYi Micro Hei', sans-serif;">In 2016, researchers from Google DeepMind, London, published their results related to "<b><a href="https://arxiv.org/pdf/1509.02971.pdf" class="md-link">Continuos Control with Deep Reinforcement Learning</a></b>" in ICLR. The learning framework is based on the <b>actor-critic network</b></p> 
<center>
            <div class="image-wrapper">
                <a class ="image-popup" href="https://nav74neet.github.io/media/blog/ac_network.jpg" title="ac">
                    <img src="https://nav74neet.github.io/media/blog/ac_network.jpg" alt="ac" align="middle">
                </a>
                <center>
                <p class="image-caption" style="font-size:10px; text-align:center;">
                    Fig.4 Actor-Critic network.
                </p>
                </center>
            </div>
</center>

<!-- <img src="https://nav74neet.github.io/media/blog/ac_network.jpg" alt="AC" align="center">
<figcaption><font size="0.5" align="center">Actor-Critic network model.</font></figcaption>
</p> -->

### Initial learning phase.
<div class="row">
<div class="col-md-7">
<p style="text-align:justify; font-family: 'Merriweather', 'Hiragino Sans GB', 'Microsoft YaHei', 'WenQuanYi Micro Hei', sans-serif;"> At the initial stages of the learning to walk, the robot takes baby steps and eventually fails a lot of times by falling down. The reward function is modelled accordingly by this inference so that the robot moves forward while maintaining constant waist-height from the ground to maximize the reward for each iteration.
</p>
</div>
<div class="col-md-5">
      <img class="rimg" src="{{ site.github.url }}/media/biped_training.gif"/>
      <figcaption><font size="0.5">Fig.5 Initial learning phase.</font></figcaption>
  </div>
</div>


<!-- <img src="https://nav74neet.github.io/media/biped_training.gif" alt="DDPG" style="border:2px solid black;" align="middle">
<figcaption><font size="0.5" align="center">The initial learning phase.</font></figcaption>
</p> -->

## Results & comparison with an actual human gait.
<div class="row">
    <div class="col-md-7">
      <p style="text-align:justify; font-family: 'Merriweather', 'Hiragino Sans GB', 'Microsoft YaHei', 'WenQuanYi Micro Hei', sans-serif;"> Our model learned to generate a stable walking gait after roughly over 41 hours of training (using a Nvidia GeForce GTX 1050 Ti GPU). After training the model in simulation, it was observed that, with a proper shaped reward function, the robot achieved faster walking (or even rendered a running gait) with an average speed of 0.83 m/s. Though the problem of convergence of the learning still persists with the DDPG algorithm, we were able to get a bipedal gait which was similar to that of a human's. The human gait was synthezised & derived using optical motion capture camera setup at the Advanced Flight Simulation Laboratory, Dept. of Aerospace Engineering, IISc. The half-body marker placement convention was adopted while recording the subject's walking gait. The hip & knee joint rotations of the human subject and the bipedal walker were gathered and the subsequent plots were generated, depicting similarities in patterns.</p> 
    </div>
  <div class="col-md-5">
      <img class="rimg" src="{{ site.github.url }}/media/biped_trained.gif"/>
      <figcaption><font size="0.5">Fig.6 The DDPG trained model - stable walking.</font></figcaption>
  </div>
  <br>
  <div class="col-md-5">
      <img class="rimg" src="{{ site.github.url }}/media/blog/motive2.gif"/>
      <figcaption><font size="0.5">Fig.7 Motion capture recording of a human walking gait.</font></figcaption>
  </div>
</div>

<div class="row">
  <div class="col-md-6">
      <img class="rimg" src="{{ site.github.url }}/media/blog/biped_hip.png"/>
      <p class="image-caption" style="font-size:10px; text-align: center;">
                    Fig.8 Hip joint rotation trajectory for biped robot.
      </p>
  </div>
  <div class="col-md-6">
      <img class="limg" src="{{ site.github.url }}/media/blog/human_hip.png"/>
      <p class="image-caption" style="font-size:10px; text-align: center;">
                    Fig.9 Hip joint rotation trajectory for human walking.
      </p>
  </div>
</div>
<br>
<div class="row">
  <div class="col-md-6">
      <img class="rimg" src="{{ site.github.url }}/media/blog/biped_knee.png"/>
      <p class="image-caption" style="font-size:10px; text-align: center;">
                    Fig.10 Knee joint rotation trajectory for biped robot.
      </p>
  </div>
  <div class="col-md-6">
      <img class="limg" src="{{ site.github.url }}/media/blog/human_knee.png"/>
      <p class="image-caption" style="font-size:10px; text-align: center;">
                    Fig.11 Hip joint rotation trajectory for human walking.
      </p>
  </div>
</div>
<!-- <center>
            <div class="image-wrapper">
                <a class ="image-popup" href="https://nav74neet.github.io/media/trained.gif" title="DDPG">
                    <img src="https://nav74neet.github.io/media/trained.gif" alt="DDPG" style="border:2px solid black;" align="left">
                </a>
                <p class="image-caption" style="font-size:14px; text-align: center;">
                    Fig. 2 Stable Bipedal walking robot.
                </p>
            </div>
</center> -->

<!-- <div class="row">
<div class="col-md-5">
      <img class="rimg" src="{{ site.github.url }}/media/blog/motive.gif"/>
</div> 
</div> -->

## Research outputs & future works

<p style="text-align:justify; font-family: 'Merriweather', 'Hiragino Sans GB', 'Microsoft YaHei', 'WenQuanYi Micro Hei', sans-serif;"><b>[1]</b> The research manuscript detailing our reinforcement learning approach for bipedal walking robot & corresponding results have been documented and can be accessed here: <b>Kumar, Arun, Navneet Paul, and S. N. Omkar. "<a href="https://arxiv.org/abs/1807.05924" class="md-link">Bipedal Walking Robot using Deep Deterministic Policy Gradient.</a>" arXiv:1807.05924 (2018)</b>.</p> 

<p style="text-align:justify; font-family: 'Merriweather', 'Hiragino Sans GB', 'Microsoft YaHei', 'WenQuanYi Micro Hei', sans-serif;"><b>[2]</b> Simulation framework (using ROS & Gazebo) for the planar bipedal walking robot is opensourced through GitHub <b><a href="https://github.com/nav74neet/ddpg_biped" class="md-link">here</a></b>. You can explore the recent state of the art RL algorithms such as <a href="https://blog.openai.com/openai-baselines-ppo/" class="md-link">PPO</a> or  <a href="https://arxiv.org/abs/1502.05477" class="md-link">TRPO</a> for generating walking patterns for the biped robot.</p>

## Other useful resources:

<p style="text-align:justify; font-family: 'Merriweather', 'Hiragino Sans GB', 'Microsoft YaHei', 'WenQuanYi Micro Hei', sans-serif;">There are several similar opensource resources available online which use machine learning techniques for training an agent to walk (or possibily, even run). I have made a mention of a few below.</p>

<div class="row">
    <div class="col-md-7">
      <p style="text-align:justify; font-family: 'Merriweather', 'Hiragino Sans GB', 'Microsoft YaHei', 'WenQuanYi Micro Hei', sans-serif;"> <b>[1] NeurIPS "Learning to run" challenge:</b> Over the past two years NeurIPS, [formerly known as NIPS] has been hosting the online "Learning to run" challenge, where the contestents have to train a musculoskeletal model in Opensim simulation environment to run at a particular speed and avoid obstacles at the same time.[<a href="https://www.crowdai.org/challenges/nips-2018-ai-for-prosthetics-challenge" class="md-link">NeurIPS, 2018 challenge</a>][<a href="https://github.com/stanfordnmbl/osim-rl" class="md-link">repository</a>]</p> 
    </div>
  <div class="col-md-5">
      <img class="rimg" src="{{ site.github.url }}/media/blog/NIPS.gif"/>
      <figcaption><font size="0.5">The musculoskeletal model in Opensim environment.</font></figcaption>
  </div>
<!-- <p style="text-align:justify">The reason we opted not to go forward with CPG based controllers was infact due to the random pattern generated by the network which did not account for the stability of the system. To consider the factor of dynamic stability of the system, we incorporated a balancing pendulum on top of the waist section of the walker with roll, pitch & yaw (to act as a spine). The balancing control would be taken care of by reinforcement learning where, the top pendulum tries to figure out the optimum pitch, yaw & roll values to stay upright. The CPG network will deal with the generation of walking pattern for the lower torso. But it was the case of easier said than done. </p>  -->
